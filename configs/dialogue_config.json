{
  "model": {
    "vocab_size": 50257,
    "hidden_dim": 768,
    "num_layers": 12,
    "num_heads": 12,
    "dropout": 0.1,
    "max_seq_length": 1024,
    "primes": [23, 29, 31, 37, 41, 43, 47],
    "max_iterations": 10,
    "entropy_threshold": 0.01,
    "phase_factor": 0.5,
    "extensions": {
      "extensions_enabled": false,
      "enable_memory": false,
      "enable_multimodal": false,
      "enable_quantum": false
    }
  },
  "training": {
    "batch_size": 4,
    "eval_batch_size": 4,
    "learning_rate": 5e-5,
    "weight_decay": 0.01,
    "max_epochs": 3,
    "training_type": "dialogue",
    "learning_mode": "adaptive",
    "device": null,
    "use_mixed_precision": true,
    "optimizer": "adamw",
    "max_grad_norm": 1.0,
    "accumulation_steps": 1,
    "lr_scheduler": "cosine",
    "warmup_steps": 0,
    "logging_steps": 10,
    "save_steps": 0,
    "eval_steps": 0,
    "save_every_epoch": true,
    "disable_optimizer_saving": false,
    "output_dir": "runs/dialogue_model",
    "seed": 42
  },
  "data": {
    "dataset_name": "daily_dialog",
    "tokenizer_name": "gpt2",
    "train_file": "examples/simple_dialogue.json",
    "validation_file": null,
    "test_file": null,
    "data_path": null,
    "max_length": 512,
    "stride": 256,
    "preprocessing_num_workers": 4,
    "cache_dir": ".cache",
    "return_tensors": "pt",
    "system_prompt": "You are a helpful assistant."
  }
}